{
  "metadata": {
    "repository": "https://github.com/b08x/video-chapter-automater",
    "generated_at": "2025-12-31T19:48:53.466227",
    "page_count": 11
  },
  "pages": [
    {
      "id": "introduction",
      "title": "Introduction",
      "content": "<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n- [src/video_chapter_automater/output/manager.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/output/manager.py)\n- [src/video_chapter_automater/setup_wizard.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/setup_wizard.py)\n- [src/video_chapter_automater/main.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/main.py)\n- [src/video_chapter_automater/pipeline/config.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/pipeline/config.py)\n- [src/video_chapter_automater/pipeline/orchestrator.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/pipeline/orchestrator.py)\n- [src/video_chapter_automater/app_paths.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/app_paths.py)\n- [src/video_chapter_automater/preprocessing/base.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/preprocessing/base.py)\n</details>\n\n# Introduction\n\nVideo Chapter Automater is a structured system designed for the automated generation of video chapters through AI-powered scene detection and hardware-accelerated preprocessing. The system is architected around a multi-stage pipeline that coordinates video re-encoding, audio extraction, and scene analysis. It relies on a centralized management of system paths and output directories to maintain state across complex processing tasks.\n\nThe system's behavior is governed by a `PipelineOrchestrator` that manages `Stage` execution based on a `PipelineConfig`. This orchestration ensures that dependencies between media extraction and analysis are satisfied, while a `SetupWizard` handles the initial environment validation and GPU capability detection.\n\n## System Architecture and Orchestration\n\nThe core of the system is the pipeline mechanism, which transitions a raw video file through various states of extraction and metadata generation.\n\n### Pipeline Execution Flow\nThe `PipelineOrchestrator` is the primary controller. It initializes the `OutputManager` and iterates through a list of `StageConfig` objects. The execution mode determines how failures are handled, specifically through `SEQUENTIAL` or `RESILIENT` modes.\n\n```mermaid\ngraph TD\n    A[Input Video] --> B[PipelineOrchestrator]\n    B --> C{Execution Mode}\n    C -->|SEQUENTIAL| D[Execute Stage 1]\n    D -->|Success| E[Execute Stage 2]\n    D -->|Failure| F[Halt Pipeline]\n    C -->|RESILIENT| G[Execute All Stages]\n    G --> H[Collect Results]\n    E --> I[Generate Manifest]\n    H --> I\n    I --> J[Output Directory]\n```\n*Note: The orchestrator explicitly manages the lifecycle of the process, from source copying to manifest generation.*\nSources: `[src/video_chapter_automater/pipeline/orchestrator.py:L26-L85]`, `[src/video_chapter_automater/pipeline/config.py:L31-L45]`\n\n### Component Responsibilities\n\n| Component | Responsibility | Key Interactions |\n| :--- | :--- | :--- |\n| `OutputManager` | Centralized I/O and directory structure maintenance. | Interacts with `PipelineOrchestrator` to store stage outputs. |\n| `PipelineOrchestrator` | Manages the lifecycle of preprocessing stages. | Coordinates `Stage` objects and `OutputManager`. |\n| `SetupWizard` | Environment validation and user preference configuration. | Detects GPU via `GPUDetector` and sets `UserPreferences`. |\n| `ApplicationPaths` | Platform-agnostic path resolution (Config, Data, Cache). | Used by `EnhancedVideoProcessor` for state persistence. |\n| `PreprocessingOperation` | Abstract contract for specific tasks (Encoding, Extraction). | Implemented by specific logic in the pipeline. |\n\nSources: `[src/video_chapter_automater/output/manager.py:L106-L135]`, `[src/video_chapter_automater/pipeline/orchestrator.py:L10-L30]`, `[src/video_chapter_automater/app_paths.py:L21-L45]`\n\n## Output and State Management\n\nThe system utilizes a rigid directory structure to organize the artifacts of the preprocessing stages. This structure is enforced by the `OutputManager`, which maps `OutputType` enums to specific subdirectories.\n\n### Directory Structure Mapping\nThe system automatically generates a structured hierarchy under a base directory (defaulting to `./vca_output`).\n\n- **video/**: Re-encoded video files.\n- **audio/**: Extracted 16kHz mono WAV files.\n- **scenes/**: PNG images organized by video name.\n- **chapters/**: FFmpeg metadata format markers.\n- **metadata/**: JSON manifests and statistics.\n\nSources: `[src/video_chapter_automater/output/manager.py:L13-L55]`, `[src/video_chapter_automater/output/manager.py:L137-L148]`\n\n### The Manifest Mechanism\nUpon successful pipeline completion, the orchestrator generates a manifest. This file acts as the \"source of truth\" for a specific processing run, containing timestamps, paths to all generated outputs, and processing statistics.\n\n```python\n# Conceptual structure of manifest generation\ndef _generate_manifest(self, input_path: Path, stage_results: List[StageResult], duration: float):\n    # Aggregates metadata from all stages into a single JSON file\n    pass\n```\nSources: `[src/video_chapter_automater/pipeline/orchestrator.py:L80-L85]`, `[src/video_chapter_automater/output/manager.py:L57-L65]`\n\n## Initialization and Configuration\n\nThe `SetupWizard` and `ApplicationPaths` handle the system's \"pre-flight\" state. The `SetupWizard` uses a TUI (Text User Interface) to guide users through GPU detection and dependency installation.\n\n### Configuration Hierarchy\n1. **UserPreferences**: High-level settings (GPU preference, output format, parallel processing).\n2. **PipelineConfig**: Low-level execution settings (Stage ordering, cleanup policies, project naming).\n3. **StageConfig**: Specific parameters for individual operations (Scene detection thresholds, codecs).\n\nSources: `[src/video_chapter_automater/setup_wizard.py:L62-L85]`, `[src/video_chapter_automater/pipeline/config.py:L55-L75]`\n\n### Observed Structural Inconsistencies\nThe system presents an interesting contradiction in its parallel processing implementation. While `ExecutionMode.PARALLEL` is defined in the configuration and exposed in the `UserPreferences`, the `PipelineOrchestrator` contains a fucking placeholder that defaults parallel requests back to sequential execution. This reveals a design that is architecturally \"ready\" for concurrency but functionally restricted to synchronous flows in its current iteration.\n\nSources: `[src/video_chapter_automater/pipeline/config.py:L43-L45]`, `[src/video_chapter_automater/pipeline/orchestrator.py:L72-L75]`\n\n## Conclusion\n\nThe Video Chapter Automater is a modular, pipeline-driven system that prioritizes structural organization and hardware utilization. By decoupling the preprocessing operations from the orchestration logic, the system maintains a clear flow of data from raw video to structured metadata. Its reliance on centralized path management and automated environment setup indicates a focus on reproducibility and cross-platform stability, despite some internal components remaining in a transitional state regarding parallel execution.",
      "filePaths": [
        "README.md",
        "src/video_chapter_automater/__init__.py"
      ],
      "importance": "high",
      "relatedPages": [
        "getting-started",
        "core-architecture"
      ]
    },
    {
      "id": "getting-started",
      "title": "Getting Started",
      "content": "<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n- [src/video_chapter_automater/setup_wizard.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/setup_wizard.py)\n- [src/video_chapter_automater/cli.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/cli.py)\n- [src/video_chapter_automater/main.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/main.py)\n- [src/video_chapter_automater/output/manager.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/output/manager.py)\n- [src/video_chapter_automater/pipeline/config.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/pipeline/config.py)\n- [demo_setup.py](https://github.com/b08x/video-chapter-automater/blob/main/demo_setup.py)\n</details>\n\n# Getting Started\n\n## Introduction\nThe \"Getting Started\" phase of the Video Chapter Automater (VCA) is a multi-stage initialization process designed to align hardware capabilities with software requirements. The system employs a TUI-based setup wizard to bridge the gap between raw video files and the structured pipeline execution. This phase functions as a gatekeeper, ensuring that dependencies like FFmpeg and GPU drivers are present before any heavy lifting begins.\n\n## System Entry Points and Orchestration\nInitialization occurs through two primary paths: the interactive `SetupWizard` for environment configuration and the `CLI` for execution. The `SetupWizard` acts as a stateful guide, while the `CLI` serves as the stateless entry point for processing.\n\n### Execution Flow\nThe following diagram illustrates the sequence from initial command to system readiness.\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant CLI as cli.py\n    participant Wizard as setup_wizard.py\n    participant GPU as gpu_detection.py\n    participant Output as manager.py\n\n    User->+CLI: vca --setup\n    CLI->+Wizard: run_setup_wizard()\n    Wizard->+GPU: detect_gpu_capabilities()\n    GPU-->>-Wizard: ProcessingMode (auto/nvidia/cpu)\n    Wizard->+Output: ensure_structure()\n    Output-->>-Wizard: Directories Created\n    Wizard-->>-CLI: success: bool\n    CLI->-User: Setup Complete\n```\nSources: [src/video_chapter_automater/cli.py:#L30-L45](), [src/video_chapter_automater/setup_wizard.py:#L68-L85]()\n\n## Configuration and Preferences\nThe system relies on a `UserPreferences` dataclass to persist environmental state. Curiously, while the system promotes \"automation,\" it requires significant manual validation during this phase to ensure the `scene_detection_threshold` and `gpu_preference` are sane.\n\n| Field | Type | Default | Purpose |\n| :--- | :--- | :--- | :--- |\n| `installation_type` | Enum | STANDARD | Defines dependency depth |\n| `gpu_preference` | str | \"auto\" | Overrides hardware detection |\n| `output_format` | str | \"mp4\" | Global target container |\n| `scene_detection_threshold` | float | 30.0 | Sensitivity for chapter breaks |\n\nSources: [src/video_chapter_automater/setup_wizard.py:#L88-L105](), [src/video_chapter_automater/pipeline/config.py:#L85-L105]()\n\n## The Setup Wizard Mechanism\nThe `SetupWizard` is structured as a series of `SetupStep` enumerations. It utilizes the `rich` library to provide visual feedback during high-latency operations like dependency installation and GPU detection.\n\n### Setup Steps Logic\n1.  **Welcome**: Displays branding and overview.\n2.  **System Check**: Validates Python environment and external binaries.\n3.  **GPU Detection**: Probes for NVIDIA (CUDA), Intel, or CPU fallbacks.\n4.  **Configuration**: Captures user-defined paths and processing thresholds.\n5.  **Validation**: Performs a dry-run of the output directory creation.\n\nSources: [src/video_chapter_automater/setup_wizard.py:#L75-L85](), [demo_setup.py:#L55-L75]()\n\n## Output Directory Architecture\nA critical part of \"Getting Started\" is the instantiation of the `OutputManager`. The system enforces a rigid directory structure to prevent file collision during the multi-stage pipeline. It's a bit of a rigid setup\u2014if the directory isn't there, the system just forces it into existence.\n\n```mermaid\ngraph TD\n    Base[vca_output/] --> Video[video/]\n    Base --> Audio[audio/]\n    Base --> Scenes[scenes/]\n    Base --> Chapters[chapters/]\n    Base --> Metadata[metadata/]\n    Base --> Logs[logs/]\n```\nSources: [src/video_chapter_automater/output/manager.py:#L95-L115]()\n\n### Structural Observation\nThe `OutputManager` creates a `README.txt` within the output directory (Sources: [src/video_chapter_automater/output/manager.py:#L140]()). This serves as a runtime manifest, explaining the naming convention `{video_name}{suffix}.{extension}`. The system effectively self-documents its output as it initializes.\n\n## Pipeline Initialization\nOnce the environment is validated, the `PipelineConfig` is built. This configuration defines the `ExecutionMode` (SEQUENTIAL, PARALLEL, or RESILIENT). While the code defines a `PARALLEL` mode, the orchestrator notes it as a \"future enhancement,\" effectively defaulting to sequential execution despite the configuration's apparent flexibility. This is a classic architectural \"placeholder\" that adds complexity without current functional utility.\n\n```python\n# From src/video_chapter_automater/pipeline/config.py\nclass ExecutionMode(Enum):\n    SEQUENTIAL = \"sequential\"\n    PARALLEL = \"parallel\" # Future enhancement\n    RESILIENT = \"resilient\"\n```\nSources: [src/video_chapter_automater/pipeline/config.py:#L35-L45](), [src/video_chapter_automater/pipeline/orchestrator.py:#L85-L95]()\n\n## Conclusion\n\"Getting Started\" in VCA is a process of transitioning from a clean environment to a stateful, hardware-aware processing engine. It centers on the `SetupWizard` for configuration and the `OutputManager` for structural readiness. The system's reliance on `rich` for TUI feedback indicates a design priority on user interaction during the volatile setup phase, ensuring that the underlying pipeline has a stable foundation of directories and validated hardware paths.",
      "filePaths": [
        "src/video_chapter_automater/setup_wizard.py",
        "src/video_chapter_automater/cli.py"
      ],
      "importance": "high",
      "relatedPages": [
        "configuration-system",
        "docker-integration"
      ]
    },
    {
      "id": "core-architecture",
      "title": "Core Architecture",
      "content": "<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n- [src/video_chapter_automater/pipeline/orchestrator.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/pipeline/orchestrator.py)\n- [src/video_chapter_automater/pipeline/config.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/pipeline/config.py)\n- [src/video_chapter_automater/output/manager.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/output/manager.py)\n- [src/video_chapter_automater/cli_pipeline.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/cli_pipeline.py)\n- [src/video_chapter_automater/main.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/main.py)\n- [src/video_chapter_automater/preprocessing/base.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/preprocessing/base.py)\n- [src/video_chapter_automater/setup_wizard.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/setup_wizard.py)\n</details>\n\n# Core Architecture\n\n## Introduction\nThe Core Architecture of the Video Chapter Automater is built around a structured, multi-stage preprocessing pipeline designed to transform raw video input into segmented, metadata-rich assets. The system employs a decoupled orchestration pattern where a central `PipelineOrchestrator` manages the lifecycle of various `PreprocessingOperation` implementations. The architecture prioritizes GPU acceleration, perceptual hashing for scene deduplication, and a hierarchical output management system to maintain data integrity across complex workflows.\n\nSources: [src/video_chapter_automater/pipeline/orchestrator.py](), [src/video_chapter_automater/cli_pipeline.py]()\n\n## Pipeline Orchestration and Execution\nThe system's execution logic is encapsulated within the `PipelineOrchestrator`, which consumes a `PipelineConfig` to determine the sequence and behavior of processing stages. The orchestrator supports different execution modes, though the current implementation reveals a heavy reliance on sequential processing even when parallel modes are defined.\n\n### Execution Flow\nThe following sequence diagram illustrates the lifecycle of a video processing request through the orchestrator.\n\n```mermaid\nsequenceDiagram\n    participant CLI as cli_pipeline.py\n    participant ORCH as PipelineOrchestrator\n    participant OM as OutputManager\n    participant STAGE as Stage/Operation\n\n    CLI->+ORCH: execute(input_path)\n    ORCH->+OM: ensure_structure()\n    OM-->>-ORCH: Structure Ready\n    \n    loop For each Stage in Config\n        ORCH->+STAGE: execute(input_path)\n        alt Success\n            STAGE-->>ORCH: PreprocessingResult (Success)\n        else Failure\n            STAGE-->>-ORCH: PreprocessingResult (Failure)\n            Note over ORCH: Check stop_on_error flag\n        end\n    end\n\n    ORCH->+OM: generate_manifest()\n    OM-->>-ORCH: Manifest Written\n    ORCH-->>-CLI: PipelineResult\n```\nSources: [src/video_chapter_automater/pipeline/orchestrator.py:#L58-L115](), [src/video_chapter_automater/cli_pipeline.py:#L130-L150]()\n\n### Component Responsibilities\n| Component | Primary Responsibility | Key Interactions |\n| :--- | :--- | :--- |\n| `PipelineOrchestrator` | Manages the execution flow and state of processing stages. | `PipelineConfig`, `OutputManager`, `Stage` |\n| `OutputManager` | Enforces a rigid directory structure and handles file persistence. | `OutputType`, `Path` |\n| `PipelineConfig` | Defines the desired state of the pipeline, including enabled stages and hardware preferences. | `PipelineStage`, `ExecutionMode` |\n| `PreprocessingOperation` | Abstract interface for specific tasks like encoding or scene extraction. | `PreprocessingResult` |\n\nSources: [src/video_chapter_automater/pipeline/orchestrator.py:#L25-L50](), [src/video_chapter_automater/output/manager.py:#L105-L135](), [src/video_chapter_automater/preprocessing/base.py:#L45-L75]()\n\n## Data Management and Output Structure\nThe `OutputManager` acts as the system's file system authority. It creates a standardized hierarchy to separate different types of artifacts. This structure is not merely a convenience but a functional requirement for downstream chapter generation.\n\n### Directory Hierarchy\nThe system enforces the following structure under the base `vca_output` directory:\n- `video/`: Re-encoded video files.\n- `audio/`: Extracted 16kHz mono WAV files.\n- `scenes/`: Deduplicated scene images organized by video name.\n- `chapters/`: FFmpeg metadata format markers.\n- `metadata/`: JSON manifests and processing statistics.\n- `logs/`: Debug and execution logs.\n\nSources: [src/video_chapter_automater/output/manager.py:#L110-L125](), [src/video_chapter_automater/output/manager.py:#L15-L45]()\n\n### Manifest Generation\nUpon successful completion, the orchestrator triggers the generation of a JSON manifest. This file serves as the \"source of truth\" for the processed project, mapping stage results to specific file paths.\n\n```python\n# src/video_chapter_automater/pipeline/orchestrator.py\ndef _generate_manifest(self, input_path: Path, stage_results: List[StageResult], duration: float) -> None:\n    outputs = {}\n    stats = {}\n    for result in stage_results:\n        if result.output_path:\n            outputs[result.stage.value] = result.output_path\n        stats[result.stage.value] = {\n            \"status\": result.status.value,\n            \"duration\": result.duration,\n        }\n    self.output_manager.generate_manifest(\n        video_name=input_path.stem,\n        outputs=outputs,\n        stats={\"total_duration\": duration, \"stages\": stats}\n    )\n```\nSources: [src/video_chapter_automater/pipeline/orchestrator.py:#L210-L235]()\n\n## Operational Tendencies and Observed Inconsistencies\nThe architecture reveals a modular design that is frequently undermined by tight coupling in the CLI and configuration layers.\n\n1.  **Pseudo-Parallelism**: While `ExecutionMode.PARALLEL` exists in the codebase, the orchestrator's implementation of it simply redirects to `_execute_sequential`. The system claims to support concurrent stages, but the logic for managing thread-safe access to the `OutputManager` or handling race conditions during scene extraction is nowhere to be found. It\u2019s a fucking placeholder that presents as a feature.\n    - Sources: [src/video_chapter_automater/pipeline/orchestrator.py:#L100-L105](), [src/video_chapter_automater/pipeline/config.py:#L40-L45]()\n\n2.  **Resilience vs. Failure**: The `RESILIENT` mode is intended to continue execution despite stage failures. However, because subsequent stages (like scene extraction) often depend on the output of previous stages (like video re-encoding), \"resilience\" in this context is often a path to a cascade of empty outputs.\n    - Sources: [src/video_chapter_automater/pipeline/orchestrator.py:#L98-L102](), [src/video_chapter_automater/pipeline/config.py:#L46-L50]()\n\n3.  **GPU Dependency**: The system heavily promotes GPU acceleration through `gpu_detection.py`, yet the `VideoEncodingConfig` allows for arbitrary codec strings. If the setup wizard identifies an NVIDIA GPU but the user passes a CPU-only codec via the CLI, the system provides no structural validation to prevent this mismatch until the subprocess inevitably shits the bed.\n    - Sources: [src/video_chapter_automater/cli_pipeline.py:#L110-L125](), [src/video_chapter_automater/setup_wizard.py:#L75-L95]()\n\n## Conclusion\nThe Core Architecture of Video Chapter Automater is a robust, if occasionally aspirational, framework for video processing. It successfully isolates file management through the `OutputManager` and operation logic through the `PipelineOrchestrator`. Its structural significance lies in its ability to standardize the messy process of video preprocessing into a predictable manifest-driven workflow, even if some of its more advanced execution modes remain functionally dormant.\n\nSources: [src/video_chapter_automater/pipeline/orchestrator.py](), [src/video_chapter_automater/output/manager.py]()",
      "filePaths": [
        "src/video_chapter_automater/processor.py",
        "src/video_chapter_automater/main.py"
      ],
      "importance": "high",
      "relatedPages": [
        "pipeline-orchestration",
        "gpu-acceleration"
      ]
    },
    {
      "id": "pipeline-orchestration",
      "title": "Pipeline Orchestration",
      "content": "<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n- [src/video_chapter_automater/pipeline/orchestrator.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/pipeline/orchestrator.py)\n- [src/video_chapter_automater/pipeline/config.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/pipeline/config.py)\n- [src/video_chapter_automater/output/manager.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/output/manager.py)\n- [src/video_chapter_automater/cli_pipeline.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/cli_pipeline.py)\n- [tests/pipeline/test_orchestrator.py](https://github.com/b08x/video-chapter-automater/blob/main/tests/pipeline/test_orchestrator.py)\n- [src/video_chapter_automater/pipeline/__init__.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/pipeline/__init__.py)\n</details>\n\n# Pipeline Orchestration\n\nThe pipeline orchestration system provides a structured framework for executing multi-stage video preprocessing workflows. It manages the lifecycle of processing tasks\u2014ranging from video encoding to scene extraction\u2014by coordinating configuration, output directory management, and execution flow. The system is designed to transform raw video input into a set of structured assets (audio, scenes, metadata) required for downstream chapter generation.\n\nSources: `[src/video_chapter_automater/pipeline/orchestrator.py]`, `[src/video_chapter_automater/pipeline/config.py]`\n\n## Architectural Components\n\nThe architecture relies on a strict separation between configuration (what to do), orchestration (how to execute), and output management (where to store results).\n\n### Component Responsibilities\n\n| Component | Responsibility | Primary Class |\n| :--- | :--- | :--- |\n| **Configuration** | Defines stages, execution modes, and global parameters. | `PipelineConfig` |\n| **Orchestration** | Manages the execution flow and state transitions of stages. | `PipelineOrchestrator` |\n| **Output Management** | Handles directory creation, file naming, and manifest generation. | `OutputManager` |\n| **Execution Unit** | Wraps individual preprocessing operations with status tracking. | `Stage` |\n\nSources: `[src/video_chapter_automater/pipeline/config.py]`, `[src/video_chapter_automater/pipeline/orchestrator.py]`, `[src/video_chapter_automater/output/manager.py]`\n\n## Execution Flow and Data Movement\n\nThe `PipelineOrchestrator` coordinates the movement of data between stages. While the system claims to support multiple execution modes, the internal logic reveals a rigid dependency on sequential processing for manifest generation and error handling.\n\n### Pipeline Execution Sequence\n\nThe following diagram illustrates the lifecycle of a pipeline execution from the initial input to the final manifest generation.\n\n```mermaid\nsequenceDiagram\n    participant CLI as cli_pipeline\n    participant ORC as PipelineOrchestrator\n    participant OM as OutputManager\n    participant STG as Stage\n\n    CLI->+ORC: execute(input_path)\n    ORC->+OM: ensure_structure()\n    OM-->>-ORC: Done\n    \n    opt copy_source is True\n        ORC->>OM: _copy_source_file(input_path)\n    end\n\n    loop For each StageConfig\n        ORC->>+STG: execute(input_path)\n        STG-->>-ORC: StageResult\n        alt Stage Failed & stop_on_error\n            ORC-->>CLI: PipelineResult(success=False)\n        end\n    end\n\n    ORC->>+OM: generate_manifest(stats, outputs)\n    OM-->>-ORC: Path\n    ORC-->>-CLI: PipelineResult(success=True)\n```\n\nSources: `[src/video_chapter_automater/pipeline/orchestrator.py:#L128-L195]`, `[src/video_chapter_automater/cli_pipeline.py:#L120-L150]`\n\n## Configuration and Stage Management\n\nPipeline behavior is dictated by `PipelineConfig`, which aggregates multiple `StageConfig` objects. The system supports three distinct stages, though the orchestration logic implies a functional hierarchy where video encoding often precedes extraction tasks.\n\n### Available Pipeline Stages\n- **VIDEO_ENCODING**: Re-encodes video using GPU acceleration or specific codecs.\n- **AUDIO_EXTRACTION**: Extracts 16kHz mono WAV files for transcription.\n- **SCENE_EXTRACTION**: Extracts visual frames for scene detection.\n\nSources: `[src/video_chapter_automater/pipeline/config.py:#L26-L36]`, `[src/video_chapter_automater/output/manager.py:#L14-L25]`\n\n### Execution Modes\nThe `ExecutionMode` enum defines how the system handles the progression of stages:\n1. **SEQUENTIAL**: Standard one-by-one execution.\n2. **RESILIENT**: Continues execution even if individual stages fail.\n3. **PARALLEL**: Defined in code but explicitly noted as a \"future enhancement,\" effectively defaulting back to sequential execution in the current implementation.\n\nSources: `[src/video_chapter_automater/pipeline/config.py:#L39-L49]`, `[src/video_chapter_automater/pipeline/orchestrator.py:#L158-L164]`\n\n## Output Organization and Manifests\n\nThe `OutputManager` enforces a standardized directory structure. A recurring pattern in the architecture is the use of a \"project folder\" which, if provided, nests all outputs under a specific subdirectory, effectively isolating different processing runs.\n\n### Directory Structure Mapping\nThe `OutputManager` maintains a mapping between `OutputType` and physical subdirectories:\n- `video/`: Re-encoded video files.\n- `audio/`: Extracted WAV files.\n- `scenes/`: PNG frames organized by video name.\n- `metadata/`: JSON manifests and processing statistics.\n- `source/`: Optional copy of the original input file.\n\nSources: `[src/video_chapter_automater/output/manager.py:#L90-L105]`, `[src/video_chapter_automater/output/manager.py:#L123-L135]`\n\n### The Manifest Mechanism\nUpon successful completion, the orchestrator triggers `_generate_manifest`. This function aggregates `StageResult` data, including execution duration and output paths, into a JSON file stored in the `metadata/` directory. This serves as the single source of truth for the processed state of a video.\n\nSources: `[src/video_chapter_automater/pipeline/orchestrator.py:#L215-L242]`\n\n## Structural Observations and Contradictions\n\nThe orchestration logic exhibits several interesting, if somewhat irritating, structural tendencies:\n\n- **The Parallelism Illusion**: The `ExecutionMode` includes a `PARALLEL` option, yet the `PipelineOrchestrator.execute` method explicitly redirects parallel requests to `_execute_sequential`. The system architecture presents an interface for concurrency that the implementation simply ignores.\n- **Source Redundancy**: If `copy_source` is enabled, the system duplicates the input video into the `source/` subdirectory before processing. While this ensures project portability, it creates immediate storage overhead before any actual \"processing\" occurs.\n- **Manifest Dependency**: Manifest generation is coupled to the overall success of the pipeline. In `RESILIENT` mode, if a non-critical stage fails but the pipeline technically \"succeeds,\" the manifest is still generated, potentially leading to downstream consumers finding missing expected assets.\n\nSources: `[src/video_chapter_automater/pipeline/orchestrator.py:#L162-L164]`, `[tests/pipeline/test_orchestrator.py:#L150-L170]`\n\n## Summary\n\nPipeline Orchestration in this repository serves as a centralized state machine that converts high-level CLI arguments into a sequence of file-system operations. Its structural significance lies in its ability to abstract the complexities of FFmpeg and image processing into a repeatable \"Stage\" based workflow. However, the architecture is currently optimized for sequential, synchronous execution, with hooks for parallelism and resilience that are either partially implemented or functionally bypassed by the main orchestration loop.",
      "filePaths": [
        "src/video_chapter_automater/pipeline/orchestrator.py",
        "src/video_chapter_automater/pipeline/config.py",
        "src/video_chapter_automater/pipeline/stage.py"
      ],
      "importance": "medium",
      "relatedPages": [
        "preprocessing-stages"
      ]
    },
    {
      "id": "gpu-acceleration",
      "title": "GPU Acceleration & Detection",
      "content": "<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n- [src/video_chapter_automater/gpu_detection.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/gpu_detection.py)\n- [src/video_chapter_automater/preprocessing/video_encoder.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/preprocessing/video_encoder.py)\n- [src/video_chapter_automater/processor.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/processor.py)\n- [src/video_chapter_automater/cli_pipeline.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/cli_pipeline.py)\n- [src/video_chapter_automater/main.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/main.py)\n- [src/video_chapter_automater/setup_wizard.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/setup_wizard.py)\n</details>\n\n# GPU Acceleration & Detection\n\n## Introduction\nThe GPU Acceleration & Detection system in Video Chapter Automater is a diagnostic and orchestration layer designed to identify hardware capabilities and apply optimized encoding paths. It functions as a hardware abstraction layer that determines whether the system can leverage NVIDIA (NVENC) or Intel (QuickSync) hardware encoders versus falling back to software-based CPU encoding. This mechanism is central to the `vca-pipeline`, where it dictates the selection of codec strategies during the video re-encoding stage.\n\nSources: [src/video_chapter_automater/gpu_detection.py](), [src/video_chapter_automater/preprocessing/video_encoder.py]()\n\n## Hardware Detection Mechanism\nThe system utilizes the `GPUDetector` class to probe the host environment for specific vendor signatures and driver support. It primarily targets NVIDIA and Intel hardware through shell-level probes and subprocess calls.\n\n### Detection Logic Flow\nThe detection process follows a hierarchical search, prioritizing NVIDIA hardware before checking for Intel integrated graphics.\n\n```mermaid\ngraph TD\n    A[Start Detection] --> B{Check NVIDIA}\n    B -- Found --> C[Initialize GPUInfo: NVIDIA]\n    B -- Not Found --> D{Check Intel}\n    D -- Found --> E[Initialize GPUInfo: Intel]\n    D -- Not Found --> F[Set ProcessingMode: CPU_ONLY]\n    C --> G[Determine Best Mode]\n    E --> G\n    G --> H[Return ProcessingMode & FFmpeg Args]\n```\nThe `detect_all_gpus` method populates a list of `GPUInfo` objects, which include metadata such as vendor name, memory, and driver versions.\n\nSources: [src/video_chapter_automater/gpu_detection.py:#L46-L70]()\n\n### GPU Metadata Structures\n| Attribute | Type | Description |\n| :--- | :--- | :--- |\n| `vendor` | `GPUVendor` | Enum: NVIDIA, INTEL, or UNKNOWN |\n| `name` | `str` | Marketing name of the detected hardware |\n| `processing_mode` | `ProcessingMode` | Determines the specific FFmpeg acceleration path (e.g., `nvidia_gpu`) |\n\nSources: [src/video_chapter_automater/gpu_detection.py:#L21-L40]()\n\n## Integration with Video Encoding\nGPU detection results are consumed by the `VideoEncoder` and its associated `CodecStrategy` implementations. The system maps specific hardware modes to FFmpeg-compatible encoder strings.\n\n### Codec Strategy Mapping\nThe `VideoEncoder` maintains a mapping of strategy classes that handle the technical implementation of different encoding paths.\n\n| Strategy Key | Class | Target Hardware |\n| :--- | :--- | :--- |\n| `h264_nvenc` | `H264NvencStrategy` | NVIDIA GPU (H.264) |\n| `hevc_nvenc` | `HevcNvencStrategy` | NVIDIA GPU (H.265) |\n| `libx264` | `LibX264Strategy` | CPU (Software H.264) |\n\nSources: [src/video_chapter_automater/preprocessing/video_encoder.py:#L57-L63]()\n\n### The Fallback Mechanism\nA notable structural pattern is the \"interactive fallback.\" If a GPU-accelerated encoding attempt fails (e.g., due to driver issues or unsupported profiles), the `VideoEncoder` catches the `GPUEncodingError` and prompts the user to switch to CPU encoding. This reveals a dependency where the system assumes hardware capability until the actual execution phase fails.\n\n```python\n# Observed fallback logic in VideoEncoder\ntry:\n    result = self.strategy.execute(input_path, config)\nexcept GPUEncodingError:\n    if self.interactive and Confirm.ask(\"GPU encoding failed. Fallback to CPU?\"):\n        self.strategy = LibX264Strategy()\n        result = self.strategy.execute(input_path, config)\n```\n\nSources: [src/video_chapter_automater/preprocessing/video_encoder.py:#L45-L55]()\n\n## Pipeline Orchestration\nThe `vca-pipeline` CLI command uses the detection module to provide users with immediate feedback on their system's capabilities before long-running tasks begin.\n\n### Execution Sequence\nThe following sequence illustrates how the CLI coordinates with the detection and processing components:\n\n```mermaid\nsequenceDiagram\n    participant CLI as cli_pipeline.py\n    participant Det as gpu_detection.py\n    participant Proc as processor.py\n    participant Enc as video_encoder.py\n\n    CLI->+Det: detect_gpu_capabilities()\n    Det-->>-CLI: (ProcessingMode, GPUInfo, args)\n    CLI->+Proc: Initialize with Mode\n    Proc->+Enc: execute(video_file)\n    alt GPU Success\n        Enc-->>Proc: VideoEncodingResult (gpu_accelerated=True)\n    else GPU Failure\n        Enc->>Enc: Trigger CPU Fallback\n        Enc-->>-Proc: VideoEncodingResult (gpu_accelerated=False)\n    end\n    Proc-->>-CLI: Pipeline Summary\n```\n\nSources: [src/video_chapter_automater/cli_pipeline.py:#L170-L190](), [src/video_chapter_automater/processor.py:#L60-L80]()\n\n## Structural Observations\nThe architecture presents a clean separation between detection (`gpu_detection.py`) and execution (`video_encoder.py`). However, an analytical look reveals that while the `GPUDetector` is quite thorough, the actual `VideoProcessor` in `processor.py` often treats GPU acceleration as an \"all-or-nothing\" flag. This is a fucking annoying inconsistency: the system detects specific hardware details (like memory and driver versions) but often collapses that complexity into a simple boolean `gpu_acceleration` in the `ProcessingStats` dataclass.\n\nSources: [src/video_chapter_automater/main.py:#L68-L80](), [src/video_chapter_automater/gpu_detection.py:#L120-L140]()\n\n## Conclusion\nGPU Acceleration & Detection serves as the performance-critical foundation of the Video Chapter Automater. By abstracting vendor-specific FFmpeg arguments into `ProcessingMode` enums and `CodecStrategy` classes, the system ensures that high-intensity scene detection and re-encoding tasks can scale with available hardware while maintaining a mandatory (if slightly pessimistic) CPU fallback path.",
      "filePaths": [
        "src/video_chapter_automater/gpu_detection.py",
        "src/video_chapter_automater/preprocessing/strategies/codec_strategies.py"
      ],
      "importance": "high",
      "relatedPages": [
        "core-architecture"
      ]
    },
    {
      "id": "scene-detection-logic",
      "title": "Scene Detection & Chapter Conversion",
      "content": "<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n- [src/video_chapter_automater/chapconv.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/chapconv.py)\n- [src/video_chapter_automater/preprocessing/scene_extractor.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/preprocessing/scene_extractor.py)\n- [src/video_chapter_automater/output/manager.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/output/manager.py)\n- [src/video_chapter_automater/pipeline/config.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/pipeline/config.py)\n- [src/video_chapter_automater/cli_pipeline.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/cli_pipeline.py)\n</details>\n\n# Scene Detection & Chapter Conversion\n\n## 1. Introduction\nThe \"Scene Detection & Chapter Conversion\" subsystem serves as the core analytical engine of the VideoChapterAutomater. Its primary mechanism is the transformation of raw visual temporal data into structured navigational metadata. This process involves detecting content-based boundaries via `PySceneDetect`, extracting representative visual frames with perceptual hash-based deduplication, and finally serializing these temporal markers into FFmpeg-compatible metadata. The system acts as a bridge between computer vision analysis and media container standards.\n\n## 2. Scene Extraction Mechanism\nScene extraction is handled by the `SceneExtractor` class, which operates as a `PreprocessingOperation`. It relies on the `scenedetect` library to identify shifts in video content based on a configurable threshold.\n\n### 2.1 Configuration and Validation\nThe extraction process is governed by `SceneExtractionConfig`, which defines the sensitivity and output characteristics of the detection.\n\n| Attribute | Type | Default | Description |\n| :--- | :--- | :--- | :--- |\n| `num_images` | `int` | 3 | Images to extract per scene (Range: 1-9) |\n| `threshold` | `float` | 27.0 | Content detection sensitivity |\n| `dedup_threshold` | `int` | 5 | Hash similarity for deduplication (0-10) |\n| `hash_algorithm` | `str` | \"phash\" | Algorithm: phash, dhash, or whash |\n| `min_scene_length` | `float` | 0.5 | Minimum duration in seconds |\n\nSources: `[src/video_chapter_automater/preprocessing/scene_extractor.py:#L36-L55]`\n\n### 2.2 Deduplication Logic\nA notable structural pattern is the use of `PerceptualHashStrategy` to eliminate redundant frames. The system doesn't just dump every frame; it aggressively filters \"bullshit\" near-duplicate images that provide no unique visual information for a chapter. This deduplication is critical when `num_images` is set high, ensuring the `scenes/` directory remains somewhat sane.\n\nSources: `[src/video_chapter_automater/preprocessing/scene_extractor.py:#L105-L115]`\n\n## 3. Chapter Conversion and Serialization\nOnce scenes are detected, the `ChapterConverter` translates the CSV-based output of the detector into a format FFmpeg can embed.\n\n### 3.1 Data Flow: CSV to FFmpeg\nThe conversion logic follows a strict linear progression:\n1. **Parse**: `parse_pyscenedetect_csv` reads the scene boundaries.\n2. **Model**: Data is encapsulated into `ChapterEntry` objects.\n3. **Format**: `to_ffmpeg_format` generates the `;FFMETADATA1` compliant string.\n\n```mermaid\ngraph TD\n    A[PySceneDetect CSV] -->|Parse| B(ChapterEntry List)\n    B -->|Iterate| C{Format Logic}\n    C -->|Escape Title| D[FFmpeg Metadata Block]\n    D -->|Join| E[Final Metadata File]\n```\n\nSources: `[src/video_chapter_automater/chapconv.py:#L32-L65]`\n\n### 3.2 The FFmpeg Metadata Schema\nThe `ChapterEntry` class ensures that timebases are normalized to `1/1000` (milliseconds), which is the standard expected by FFmpeg for chapter markers. It also handles the escaping of characters like `=` and newlines in titles to prevent the metadata parser from shitting itself during the embedding phase.\n\nSources: `[src/video_chapter_automater/chapconv.py:#L39-L55]`\n\n## 4. System Interaction and Pipeline Integration\nThe scene detection stage is integrated into a larger `PipelineConfig` as a `PipelineStage`.\n\n### 4.1 Pipeline Sequence\nThe `cli_pipeline.py` orchestrates the flow. While scene extraction is technically independent, the pipeline structure suggests it often follows video re-encoding to ensure the detector works on a consistent codec/bitrate.\n\n```mermaid\nsequenceDiagram\n    participant CLI as CLI/Orchestrator\n    participant SE as SceneExtractor\n    participant OM as OutputManager\n    participant CC as ChapterConverter\n\n    CLI->>SE: Execute(video_path)\n    activate SE\n    SE->>OM: Request Subdir (scenes/)\n    SE-->>SE: Detect Scenes & Save Images\n    SE-->>CLI: SceneExtractionResult\n    deactivate SE\n    \n    CLI->>CC: pyscenedetect_to_ffmpeg(csv_path)\n    CC-->>CLI: FFmpeg Metadata String\n```\n\nSources: `[src/video_chapter_automater/cli_pipeline.py:#L45-L55]`, `[src/video_chapter_automater/pipeline/config.py:#L22-L30]`\n\n### 4.2 Output Organization\nThe `OutputManager` provides a centralized structure for the artifacts generated during this process. Despite the modularity of the code, the `OutputManager` enforces a rigid directory hierarchy that the `SceneExtractor` must respect.\n\n| Output Type | Directory | Content Description |\n| :--- | :--- | :--- |\n| `SCENES` | `scenes/{video_name}/` | Deduplicated PNG/JPG frame captures |\n| `CHAPTERS` | `chapters/` | FFmpeg metadata files |\n| `METADATA` | `metadata/` | JSON manifests and processing stats |\n\nSources: `[src/video_chapter_automater/output/manager.py:#L100-L115]`\n\n## 5. Structural Observations\nThe system exhibits a \"resilient yet dependent\" architecture. For instance, `ChapterConverter` is designed as a standalone utility, yet its primary input method `parse_pyscenedetect_csv` is hard-coded to expect the specific CSV headers produced by `PySceneDetect`. If the detector's output format changes by even a single column header, the \"independent\" converter becomes useless. \n\nFurthermore, the `SceneExtractor` includes a `SCENEDETECT_AVAILABLE` check, but the `cli.py` and `cli_pipeline.py` treat the presence of these dependencies as a prerequisite for even starting the application. This creates a facade of optionality that is contradicted by the operational reality of the CLI.\n\nSources: `[src/video_chapter_automater/chapconv.py:#L68-L75]`, `[src/video_chapter_automater/cli.py:#L35-L45]`\n\n## 6. Conclusion\nThe Scene Detection & Chapter Conversion subsystem is the primary translator of visual change into logical structure within the repository. It successfully decouples the high-level detection logic from the low-level metadata serialization, provided the intermediary CSV format remains stable. Its structural significance lies in its ability to reduce gigabytes of video data into a few kilobytes of deduplicated images and metadata markers, facilitating efficient video navigation.",
      "filePaths": [
        "src/video_chapter_automater/chapconv.py",
        "src/video_chapter_automater/preprocessing/scene_extractor.py"
      ],
      "importance": "medium",
      "relatedPages": [
        "preprocessing-stages"
      ]
    },
    {
      "id": "preprocessing-stages",
      "title": "Preprocessing Stages",
      "content": "<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n- [src/video_chapter_automater/pipeline/config.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/pipeline/config.py)\n- [src/video_chapter_automater/pipeline/orchestrator.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/pipeline/orchestrator.py)\n- [src/video_chapter_automater/pipeline/stage.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/pipeline/stage.py)\n- [src/video_chapter_automater/preprocessing/base.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/preprocessing/base.py)\n- [src/video_chapter_automater/output/manager.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/output/manager.py)\n- [src/video_chapter_automater/cli_pipeline.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/cli_pipeline.py)\n</details>\n\n# Preprocessing Stages\n\n## Introduction\nThe preprocessing stages in `video-chapter-automater` constitute a modular workflow designed to transform raw video input into structured assets required for chapter generation. This system operates as a managed pipeline where individual operations\u2014video re-encoding, audio extraction, and scene detection\u2014are encapsulated as discrete stages. The architecture enforces a strict separation between high-level orchestration, stage-specific configuration, and the underlying execution logic.\n\nSources: [src/video_chapter_automater/pipeline/config.py:#L21-L30](), [src/video_chapter_automater/pipeline/orchestrator.py:#L1-L10]()\n\n## Pipeline Architecture and Data Flow\nThe system utilizes a hierarchical structure where a `PipelineOrchestrator` manages a collection of `Stage` objects. Each stage is bound to a `PreprocessingOperation` that implements a standard interface.\n\n### Structural Components\n- **PipelineConfig**: Defines the global execution parameters, including the sequence of stages, output directories, and error handling policies.\n- **PipelineOrchestrator**: The central engine that initializes the environment, manages the `OutputManager`, and iterates through the defined stages.\n- **Stage**: A wrapper that manages the lifecycle of a single operation, handling status tracking (`PENDING`, `COMPLETED`, `FAILED`) and result reporting.\n- **OutputManager**: Provides a centralized file system abstraction, ensuring that each stage writes to a standardized directory structure (e.g., `./vca_output/video/`, `./vca_output/audio/`).\n\nSources: [src/video_chapter_automater/pipeline/config.py:#L66-L85](), [src/video_chapter_automater/pipeline/orchestrator.py:#L45-L65](), [src/video_chapter_automater/output/manager.py:#L65-L85]()\n\n### Execution Logic\nThe following diagram illustrates the sequential flow of data and control through the pipeline. While the system defines a `PARALLEL` execution mode in its configuration, the current implementation logic observed in the orchestrator focuses on `SEQUENTIAL` execution.\n\n```mermaid\ngraph TD\n    Input[Input Video Path] --> Orchestrator[PipelineOrchestrator]\n    Orchestrator --> OutMgr[OutputManager: Ensure Structure]\n    Orchestrator --> Copy[Optional: Copy Source to Output]\n    Orchestrator --> Stage1[Stage 1: Video Encoding]\n    Stage1 --> Stage2[Stage 2: Audio Extraction]\n    Stage2 --> Stage3[Stage 3: Scene Extraction]\n    Stage3 --> Result[PipelineResult]\n    \n    subgraph \"Stage Execution\"\n    Stage1 -.-> Op1[VideoEncoder]\n    Stage2 -.-> Op2[AudioExtractor]\n    Stage3 -.-> Op3[SceneExtractor]\n    end\n```\nSources: [src/video_chapter_automater/pipeline/orchestrator.py:#L100-L125](), [src/video_chapter_automater/pipeline/stage.py:#L80-L90]()\n\n## Stage Definitions and Mechanisms\nThe system recognizes three primary stages, mapped via the `PipelineStage` enumeration and implemented through specific operation classes.\n\n| Stage Identifier | Operation Class | Primary Output | Purpose |\n| :--- | :--- | :--- | :--- |\n| `VIDEO_ENCODING` | `VideoEncoder` | Re-encoded MP4 | Standardizes video format/bitrate for downstream processing. |\n| `AUDIO_EXTRACTION` | `AudioExtractor` | 16kHz Mono WAV | Extracts audio specifically optimized for transcription. |\n| `SCENE_EXTRACTION` | `SceneExtractor` | PNG Images | Captures visual transitions for chapter marker identification. |\n\nSources: [src/video_chapter_automater/pipeline/config.py:#L21-L30](), [src/video_chapter_automater/pipeline/stage.py:#L77-L82](), [src/video_chapter_automater/output/manager.py:#L14-L25]()\n\n### Configuration Inconsistencies\nA curious architectural pattern exists in `StageConfig`. While it is designed to be highly configurable, it includes a `_get_default_config()` mechanism that automatically instantiates configuration objects if none are provided. This creates a situation where a stage might \"know\" its default parameters even if the user intended to leave it unconfigured, leading to a silent fallback behavior that might mask missing user inputs.\n\nSources: [src/video_chapter_automater/pipeline/config.py:#L55-L65]()\n\n## Orchestration and Error Handling\nThe `PipelineOrchestrator` handles the transition between stages. It utilizes a `PipelineResult` object to aggregate `StageResult` data, including execution duration and success status.\n\n### Sequence of Operations\nThe orchestrator's `execute` method follows a strict protocol to ensure environmental readiness before processing.\n\n```mermaid\nsequenceDiagram\n    participant CLI as CLI/User\n    participant Orch as PipelineOrchestrator\n    participant Out as OutputManager\n    participant Stage as Stage\n    participant Op as PreprocessingOperation\n\n    CLI->>Orch: execute(input_path)\n    activate Orch\n    Orch->>Out: ensure_structure()\n    opt copy_source is True\n        Orch->>Out: copy_source_file()\n    end\n    \n    loop for each StageConfig\n        Orch->>Stage: execute(input_path)\n        activate Stage\n        Stage->>Op: validate_input(input_path)\n        Stage->>Op: execute(input_path, config)\n        Op-->>Stage: PreprocessingResult\n        Stage-->>Orch: StageResult\n        deactivate Stage\n        \n        alt stop_on_error is True AND Stage failed\n            Note over Orch: Halt Pipeline\n        end\n    end\n    \n    Orch-->>CLI: PipelineResult\n    deactivate Orch\n```\nSources: [src/video_chapter_automater/pipeline/orchestrator.py:#L100-L140](), [src/video_chapter_automater/pipeline/stage.py:#L10-L40]()\n\n### Resiliency and Failure Modes\nThe system implements two distinct error handling strategies:\n1.  **Sequential/Strict**: The pipeline halts immediately if a stage fails (`stop_on_error=True`).\n2.  **Resilient**: The pipeline continues to subsequent stages even if a previous stage fails, provided `skip_on_error` is set for that specific stage.\n\nThere is a fucking bizarre contradiction in the `PipelineConfig` where `stop_on_error` is a global flag, yet `StageConfig` also has a `skip_on_error` flag. This creates a messy hierarchy where the global setting can effectively override individual stage preferences, making the \"resilient\" mode's behavior dependent on which flag the developer remembers to check first.\n\nSources: [src/video_chapter_automater/pipeline/config.py:#L45-L55](), [src/video_chapter_automater/pipeline/config.py:#L100-L110]()\n\n## Output Management\nThe `OutputManager` is responsible for the physical organization of files. It maps `OutputType` enums to specific subdirectories.\n\n```python\n# src/video_chapter_automater/output/manager.py\nSUBDIRS = {\n    OutputType.VIDEO: \"video\",\n    OutputType.AUDIO: \"audio\",\n    OutputType.SCENES: \"scenes\",\n    OutputType.CHAPTERS: \"chapters\",\n    OutputType.METADATA: \"metadata\",\n    OutputType.LOGS: \"logs\",\n    OutputType.SOURCE: \"source\",\n}\n```\nSources: [src/video_chapter_automater/output/manager.py:#L100-L110]()\n\n## Conclusion\nThe preprocessing stages are structured as a formal pipeline that prioritizes file system organization and stage isolation. The system relies heavily on the `PipelineOrchestrator` to maintain state across disparate operations. While the architecture is modular, the interplay between global and stage-level error flags introduces a layer of complexity that requires careful configuration to ensure predictable \"resilient\" behavior. Ultimately, the stages serve to normalize diverse video inputs into a standardized set of artifacts (WAV, re-encoded MP4, PNG) stored within a strictly defined directory tree.",
      "filePaths": [
        "src/video_chapter_automater/preprocessing/audio_extractor.py",
        "src/video_chapter_automater/preprocessing/video_encoder.py",
        "src/video_chapter_automater/preprocessing/strategies/hash_strategies.py"
      ],
      "importance": "medium",
      "relatedPages": [
        "pipeline-orchestration"
      ]
    },
    {
      "id": "output-organization",
      "title": "Output Management",
      "content": "<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n- [src/video_chapter_automater/output/manager.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/output/manager.py)\n- [src/video_chapter_automater/pipeline/orchestrator.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/pipeline/orchestrator.py)\n- [src/video_chapter_automater/pipeline/config.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/pipeline/config.py)\n- [src/video_chapter_automater/preprocessing/base.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/preprocessing/base.py)\n- [src/video_chapter_automater/cli_pipeline.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/cli_pipeline.py)\n- [tests/preprocessing/test_output_organization.py](https://github.com/b08x/video-chapter-automater/blob/main/tests/preprocessing/test_output_organization.py)\n</details>\n\n# Output Management\n\n## Introduction\nOutput Management in the `video-chapter-automater` repository is a centralized mechanism designed to govern the lifecycle, organization, and validation of artifacts generated during the video preprocessing pipeline. It serves as the structural backbone for ensuring that disparate stages\u2014such as video encoding, audio extraction, and scene detection\u2014deposit their results into a predictable, hierarchical directory tree. The system relies on the `OutputManager` class to abstract filesystem operations, enforce naming conventions, and maintain a manifest of processing results.\n\n## Architectural Structure and Data Flow\nThe system architecture centralizes file operations to prevent \"spaghetti\" directory structures. The `PipelineOrchestrator` initializes the `OutputManager`, which then dictates where every downstream component must write its data.\n\n### The Output Hierarchy\nThe default root directory is `./vca_output/`, though this is configurable. Within this root, the system enforces a strict subdirectory mapping based on the `OutputType` enumeration.\n\n| Output Type | Subdirectory | Purpose |\n| :--- | :--- | :--- |\n| `VIDEO` | `video/` | Re-encoded video files (GPU accelerated) |\n| `AUDIO` | `audio/` | Extracted WAV files (16kHz mono) |\n| `SCENES` | `scenes/` | Extracted scene images organized by video name |\n| `CHAPTERS` | `chapters/` | FFmpeg metadata format marker files |\n| `METADATA` | `metadata/` | JSON manifests and processing statistics |\n| `LOGS` | `logs/` | Debug information and execution logs |\n| `SOURCE` | `source/` | Optional copies of the original input files |\n\nSources: `src/video_chapter_automater/output/manager.py:#L66-L105`, `src/video_chapter_automater/pipeline/config.py:#L56-L59`\n\n### Interaction Logic\nThe `OutputManager` is not merely a passive directory creator; it is an active validator and namer. It generates paths using a `{video_name}{suffix}.{extension}` pattern, ensuring consistency across different pipeline stages.\n\n```python\n# src/video_chapter_automater/output/manager.py\ndef get_output_path(\n    self,\n    video_name: str,\n    output_type: OutputType,\n    extension: str,\n    suffix: str = \"\"\n) -> Path:\n    subdir_name = self.SUBDIRS[output_type]\n    return self.base_dir / subdir_name / f\"{video_name}{suffix}.{extension}\"\n```\nSources: `src/video_chapter_automater/output/manager.py:#L151-L168`\n\n## Mechanism of Pipeline Integration\nThe `PipelineOrchestrator` acts as the primary consumer of the `OutputManager`. During execution, the orchestrator coordinates the flow of data from the input file through various `Stage` instances, eventually consolidating results into a final manifest.\n\n### Sequence of Output Generation\nThe following diagram illustrates how the orchestrator interacts with the output system during a standard sequential execution.\n\n```mermaid\nsequenceDiagram\n    participant CLI as CLI/Main\n    participant Orch as PipelineOrchestrator\n    participant OM as OutputManager\n    participant Stage as PipelineStage\n\n    CLI->>Orch: execute(input_path)\n    Orch->>OM: ensure_structure()\n    opt copy_source is True\n        Orch->>OM: get_subdir(SOURCE)\n        Orch->>OM: (Internal Copy Operation)\n    end\n    loop For each Stage\n        Orch->>Stage: execute(input_path)\n        Stage-->>Orch: PreprocessingResult (output_path)\n    end\n    Orch->>OM: generate_manifest(video_name, outputs, stats)\n    OM->>OM: write manifest.json\n    Orch-->>CLI: PipelineResult\n```\nSources: `src/video_chapter_automater/pipeline/orchestrator.py:#L106-L150`, `src/video_chapter_automater/pipeline/orchestrator.py:#L174-L200`\n\n## Validation and Metadata\nThe system implements a \"trust but verify\" approach to output files. The `validate_output` method checks not only for the existence of a file but also ensures it is a non-empty, valid file entity.\n\n### Manifest Generation\nUpon successful completion of all pipeline stages, the `OutputManager` (via the orchestrator) generates a JSON manifest. This file acts as the \"source of truth\" for the processing session, containing:\n- Timestamps of execution.\n- Relative paths to all generated artifacts.\n- Processing statistics (total duration, individual stage times).\n\nSources: `src/video_chapter_automater/output/manager.py:#L130-L148`, `src/video_chapter_automater/pipeline/orchestrator.py:#L189-L205`\n\n## Operational Tendencies and Observed Inconsistencies\nThe architecture presents an interesting contradiction: while the `OutputManager` is designed to be the central authority for file organization, individual preprocessing components like `VideoEncoder` and `AudioExtractor` still maintain internal logic for building FFmpeg commands that include output paths. \n\nThe system relies on an optional `output_dir` parameter passed into these components to override their default behavior of writing to the source file's directory. If the orchestrator fails to pass this directory, the \"centralized\" management breaks down, and files are scattered\u2014a shitty structural dependency that requires the orchestrator to be perfectly configured to maintain the intended order.\n\nSources: `tests/preprocessing/test_output_organization.py:#L25-L48`, `src/video_chapter_automater/pipeline/orchestrator.py:#L56-L70`\n\n## Summary\nOutput Management in this system is a robust, albeit dependency-heavy, framework for artifact preservation. By enforcing a strict subdirectory schema and providing a unified interface for path generation and validation, it ensures that the high-volume data generated by video processing (scenes, audio, re-encoded video) remains navigable and verifiable. Its structural significance lies in its role as the final arbiter of pipeline success, signaled by the creation of the execution manifest.",
      "filePaths": [
        "src/video_chapter_automater/output/manager.py"
      ],
      "importance": "medium",
      "relatedPages": [
        "pipeline-orchestration"
      ]
    },
    {
      "id": "configuration-system",
      "title": "Configuration & App Paths",
      "content": "<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n- [src/video_chapter_automater/app_paths.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/app_paths.py)\n- [src/video_chapter_automater/output/manager.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/output/manager.py)\n- [src/video_chapter_automater/setup_wizard.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/setup_wizard.py)\n- [src/video_chapter_automater/pipeline/config.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/pipeline/config.py)\n- [src/video_chapter_automater/main.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/main.py)\n- [tests/test_app_paths.py](https://github.com/b08x/video-chapter-automater/blob/main/tests/test_app_paths.py)\n</details>\n\n# Configuration & App Paths\n\n## Introduction\nThe configuration and path management system in VideoChapterAutomater serves as the structural foundation for cross-platform persistence and artifact organization. It employs a multi-layered approach: platform-aware directory resolution for system-level configuration, a centralized `ApplicationPaths` coordinator, and a specialized `OutputManager` for process-specific artifacts. The system is designed to isolate application logic from OS-specific filesystem quirks, ensuring that configuration, cache, and data remain decoupled from the execution environment.\n\nSources: `src/video_chapter_automater/app_paths.py`, `src/video_chapter_automater/output/manager.py`\n\n## Application Path Resolution\nThe `ApplicationPaths` class acts as the single source of truth for locating system directories. It leverages platform detection to resolve base paths for configuration, data, and caching.\n\n### Platform-Specific Mapping\nThe system maps the application name `video-chapter-automater` to standard OS locations.\n\n| Path Type | Linux | macOS | Windows |\n| :--- | :--- | :--- | :--- |\n| **Config** | `~/.config/` | `~/Library/Application Support/` | `%APPDATA%\\` |\n| **Data** | `~/.local/share/` | `~/Library/Application Support/` | `%LOCALAPPDATA%\\` |\n| **Cache** | `~/.cache/` | `~/Library/Caches/` | `%TEMP%\\` |\n\nSources: `src/video_chapter_automater/app_paths.py:#L55-L75`, `tests/test_app_paths.py:#L55-L85`\n\n### Structural Interaction Flow\nThe following diagram illustrates how `ApplicationPaths` provides the filesystem context to the `UserPreferences` and `EnhancedVideoProcessor`.\n\n```mermaid\ngraph TD\n    A[ApplicationPaths] -->|Provides config_file path| B[UserPreferences]\n    A -->|Provides config_dir| C[EnhancedVideoProcessor]\n    B -->|Load/Save JSON| D[config.json]\n    C -->|Initialization| A\n```\n\nSources: `src/video_chapter_automater/app_paths.py:#L31-L50`, `src/video_chapter_automater/main.py:#L77-L80`\n\n## Configuration Mechanism\nThe system utilizes a `UserPreferences` dataclass to manage application state. This structure defines the operational constraints for the video processing pipeline.\n\n### User Preferences Schema\n| Field | Type | Default | Description |\n| :--- | :--- | :--- | :--- |\n| `installation_type` | `InstallationType` | `STANDARD` | Scope of the installation |\n| `gpu_preference` | `str` | `\"auto\"` | Hardware acceleration target |\n| `output_format` | `str` | `\"mp4\"` | Target container format |\n| `scene_detection_threshold` | `float` | `30.0` | Sensitivity for scene changes |\n| `enable_gpu_acceleration` | `bool` | `True` | Toggle for hardware usage |\n\nSources: `src/video_chapter_automater/setup_wizard.py:#L74-L85`\n\nThe system enforces validation during initialization. For instance, the `scene_detection_threshold` must be non-negative, and the `output_format` is restricted to a specific whitelist. If these invariants are violated, the system raises a `ValueError`, halting the configuration flow. This is a goddamn rigid check for a system that otherwise presents itself as highly flexible.\n\nSources: `src/video_chapter_automater/setup_wizard.py:#L87-L97`\n\n## Output Directory Management\nWhile `ApplicationPaths` handles system-level persistence, the `OutputManager` manages the \"vca_output\" directory structure where processing results are stored.\n\n### Directory Hierarchy\nThe `OutputManager` enforces a strict subdirectory mapping via the `OutputType` enum:\n- `video/`: Re-encoded video files.\n- `audio/`: Extracted WAV files (16kHz mono).\n- `scenes/`: Organized by video name.\n- `chapters/`: FFmpeg metadata format.\n- `metadata/`: JSON manifests and statistics.\n- `logs/`: Debug information.\n\nSources: `src/video_chapter_automater/output/manager.py:#L105-L115`\n\n### Path Generation Sequence\nThe `OutputManager` ensures that file naming is consistent across different processing stages.\n\n```mermaid\nsequenceDiagram\n    participant P as PipelineStage\n    participant OM as OutputManager\n    participant FS as FileSystem\n\n    P->>OM: get_output_path(video_name, output_type, ext)\n    activate OM\n    OM->>OM: Resolve SUBDIRS[output_type]\n    OM->>FS: Ensure subdir exists\n    OM-->>P: Return Path(base/subdir/video_name.ext)\n    deactivate OM\n```\n\nSources: `src/video_chapter_automater/output/manager.py:#L140-L165`\n\n## Pipeline Configuration\nThe `PipelineConfig` class bridges the gap between user preferences and execution. It defines how stages (Encoding, Audio Extraction, Scene Extraction) are ordered and executed.\n\n### Execution Modes\nThe system supports three execution strategies, though the implementation reveals a functional gap where `PARALLEL` currently defaults to `SEQUENTIAL` behavior\u2014a somewhat deceptive structural placeholder.\n\n1. **SEQUENTIAL**: Standard one-by-one execution.\n2. **RESILIENT**: Continues execution even if individual stages fail.\n3. **PARALLEL**: Future enhancement (currently routes to sequential logic).\n\nSources: `src/video_chapter_automater/pipeline/config.py:#L39-L50`, `src/video_chapter_automater/pipeline/orchestrator.py:#L105-L115`\n\n## Structural Significance\nThe configuration and path management system provides a centralized, platform-agnostic interface that prevents the application from scattering files across the host system. By separating application paths (long-term config) from output paths (short-term artifacts), the architecture maintains a clean distinction between global state and per-project results. The reliance on `ApplicationPaths` for all directory resolution ensures that the entire system can be redirected (e.g., via `XDG_CONFIG_HOME`) without modifying individual components.\n\nSources: `src/video_chapter_automater/app_paths.py`, `src/video_chapter_automater/output/manager.py`, `README.md`",
      "filePaths": [
        "src/video_chapter_automater/app_paths.py",
        "src/video_chapter_automater/platform_dirs.py"
      ],
      "importance": "low",
      "relatedPages": [
        "getting-started"
      ]
    },
    {
      "id": "docker-integration",
      "title": "Docker Deployment",
      "content": "<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n- [README.md](https://github.com/b08x/video-chapter-automater/blob/main/README.md)\n- [src/video_chapter_automater/setup_wizard.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/setup_wizard.py)\n- [src/video_chapter_automater/pipeline/config.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/pipeline/config.py)\n- [src/video_chapter_automater/output/manager.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/output/manager.py)\n- [src/video_chapter_automater/pipeline/orchestrator.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/pipeline/orchestrator.py)\n</details>\n\n# Docker Deployment\n\n## Introduction\nDocker deployment in VideoChapterAutomater serves as a containerized execution environment designed to encapsulate system dependencies such as FFmpeg, PySceneDetect, and NVIDIA GPU acceleration. The mechanism relies on a host-to-container volume mapping strategy to facilitate I/O operations, ensuring that the heavy lifting of video analysis and metadata embedding occurs within a controlled, immutable runtime.\n\nSources: [README.md](), [src/video_chapter_automater/setup_wizard.py:#L175-L185]()\n\n## Container Architecture and Environment\nThe system utilizes a multi-layered approach to environment parity. While the application can run natively, the Docker configuration explicitly targets an NVIDIA CUDA base image to provide the necessary drivers and toolkits for GPU-accelerated video processing.\n\n### Dockerfile Specification\nThe container environment is built on `python:3.11-slim` (or an NVIDIA CUDA base as noted in the README) and implements the following structural components:\n- **System Dependencies**: Installation of `ffmpeg` via `apt-get`.\n- **Dependency Management**: Utilization of `uv` for fast, frozen dependency resolution via `uv.lock`.\n- **Entry Point**: The container is configured to execute the module directly using `python -m video_chapter_automater`.\n\nSources: [src/video_chapter_automater/setup_wizard.py:#L198-L220](), [README.md]()\n\n### Hardware Integration\nThe deployment architecture is heavily dependent on the NVIDIA Container Toolkit. This dependency creates a rigid requirement for host-side configuration to enable the `--gpus all` flag, which is necessary for the container to access hardware encoders/decoders.\n\nSources: [README.md]()\n\n## Data Flow and Volume Mapping\nThe containerized execution follows a strict volume mounting pattern. Because the container is transient (`--rm`), all persistent data must be mapped to the host filesystem.\n\ngraph TD\n    HostDir[Host Working Directory] -->|Mounted to /app| ContainerApp[/app inside Container]\n    ContainerApp -->|Reads| InputVideo[Input Video File]\n    ContainerApp -->|Executes| VCAProcess[VCA Processing Pipeline]\n    VCAProcess -->|Writes| OutputVideo[Output Video with Chapters]\n    OutputVideo -->|Persisted to| HostDir\n\nThe observed flow shows a fucking simple but effective mapping: the host's current working directory is mirrored to `/app`, making the container's output immediately available to the user upon process completion.\n\nSources: [README.md](), [src/video_chapter_automater/output/manager.py:#L125-L140]()\n\n## Setup and Orchestration\nThe system provides an interactive `SetupWizard` that includes a `DOCKER_ONLY` installation type. This wizard automates the verification of the Docker daemon and the generation of the environment.\n\n### Docker Setup Logic\nThe `_setup_docker_environment` method performs the following validation steps:\n1. **Binary Check**: Verifies `docker` exists in the system PATH.\n2. **Daemon Check**: Executes `docker info` to ensure the service is active.\n3. **Template Generation**: Programmatically writes the `Dockerfile` if it is missing from the local environment.\n\nSources: [src/video_chapter_automater/setup_wizard.py:#L175-L196]()\n\n### Execution Parameters\nThe following table describes the standard parameters for deploying the container:\n\n| Parameter | Function | Structural Impact |\n| :--- | :--- | :--- |\n| `--rm` | Container Removal | Ensures no leftover container state; enforces statelessness. |\n| `-it` | Interactive TTY | Allows for real-time progress bar rendering via Rich. |\n| `--gpus all` | Hardware Passthrough | Critical for GPU-based scene detection and encoding. |\n| `-v \"$(pwd)\":/app` | Bind Mount | Maps the host filesystem to the container's working directory. |\n\nSources: [README.md](), [src/video_chapter_automater/pipeline/config.py:#L100-L115]()\n\n## Operational Tendencies and Constraints\nA structural inconsistency exists between the \"Standard\" local installation and the \"Docker\" deployment. The local installation utilizes platform-specific paths (e.g., `~/.config/vca`), whereas the Docker deployment forces a flat structure within the `/app` mount. This divergence means configuration persistence behaves differently: in Docker, the configuration is often transient or must be manually injected via additional volume flags, which is a somewhat annoying gap in the otherwise automated setup.\n\nSources: [README.md](), [src/video_chapter_automater/setup_wizard.py:#L55-L70]()\n\n## Sequence of Container Execution\nThe following diagram illustrates the interaction between the host, the Docker engine, and the internal VCA Pipeline Orchestrator.\n\nsequenceDiagram\n    participant Host as User/Host OS\n    participant Docker as Docker Engine\n    participant VCA as PipelineOrchestrator\n    participant GPU as NVIDIA GPU\n\n    Host->>Docker: docker run --gpus all -v /app\n    activate Docker\n    Docker->>VCA: Initialize(input_path)\n    activate VCA\n    VCA->>GPU: Detect hardware capabilities\n    GPU-->>VCA: GPUVendor.NVIDIA detected\n    VCA->>VCA: Execute Sequential Stages\n    Note over VCA: Video -> Audio -> Scenes -> Chapters\n    VCA->>Host: Write output_with_chapters.mp4\n    deactivate VCA\n    Docker-->>Host: Process Complete (Container Exit)\n    deactivate Docker\n\nSources: [src/video_chapter_automater/pipeline/orchestrator.py:#L55-L85](), [src/video_chapter_automater/setup_wizard.py:#L180-L200]()\n\n## Conclusion\nDocker Deployment is the primary mechanism for ensuring environment consistency in VideoChapterAutomater. It abstracts the complexity of FFmpeg and GPU driver integration into a single command-line interface. While it introduces a slight overhead in terms of volume management and configuration mapping, it provides the structural isolation necessary for high-performance video processing without polluting the host system's global namespace.",
      "filePaths": [
        "README.md"
      ],
      "importance": "medium",
      "relatedPages": [
        "gpu-acceleration"
      ]
    },
    {
      "id": "cli-reference",
      "title": "CLI Reference",
      "content": "<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n- [src/video_chapter_automater/cli.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/cli.py)\n- [src/video_chapter_automater/cli_pipeline.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/cli_pipeline.py)\n- [src/video_chapter_automater/main.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/main.py)\n- [src/video_chapter_automater/pipeline/orchestrator.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/pipeline/orchestrator.py)\n- [src/video_chapter_automater/pipeline/config.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/pipeline/config.py)\n- [src/video_chapter_automater/setup_wizard.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/setup_wizard.py)\n- [src/video_chapter_automater/output/manager.py](https://github.com/b08x/video-chapter-automater/blob/main/src/video_chapter_automater/output/manager.py)\n</details>\n\n# CLI Reference\n\n## Introduction\nThe Command Line Interface (CLI) for Video Chapter Automater (VCA) serves as the primary entry point for orchestrating video preprocessing and chapter generation workflows. The system exposes two distinct operational paths: a standard entry point (`vca`) and an advanced pipeline controller (`vca-pipeline`). These interfaces manage the transition from user input to the internal execution engines, handling system validation, GPU detection, and multi-stage pipeline configuration.\n\nSources: `src/video_chapter_automater/cli.py`, `src/video_chapter_automater/cli_pipeline.py`\n\n## Command Architecture and Entry Points\n\nThe system utilizes `argparse` to define its command-line surface. The entry points are bifurcated based on the complexity of the required task.\n\n### Standard CLI (`vca`)\nThe standard CLI is designed for general-purpose execution, offering modes ranging from \"simple\" to \"enhanced\" TUI experiences. It manages high-level flags for system inspection (GPU info, configuration) and triggers the interactive setup wizard.\n\n| Argument | Function | System Impact |\n| :--- | :--- | :--- |\n| `--setup` | `run_setup_wizard()` | Launches TUI for dependency and preference configuration. |\n| `--gpu-info` | `show_gpu_info()` | Invokes GPU detection logic to report hardware capabilities. |\n| `--simple` | Processing Mode Flag | Disables Rich TUI for minimal output. |\n| `video_file` | Positional Argument | The primary input path for the `VideoProcessor`. |\n\nSources: `src/video_chapter_automater/cli.py:#L27-L55`, `src/video_chapter_automater/cli.py:#L66-L85`\n\n### Pipeline CLI (`vca-pipeline`)\nThe pipeline CLI provides granular control over the `PipelineOrchestrator`. It allows users to toggle specific stages such as video re-encoding, audio extraction, and scene detection.\n\n```mermaid\ngraph TD\n    A[vca-pipeline command] --> B{create_parser}\n    B --> C[argparse.Namespace]\n    C --> D[build_pipeline_config]\n    D --> E[PipelineConfig Object]\n    E --> F[PipelineOrchestrator]\n    F --> G[Execute Stages]\n```\n\nSources: `src/video_chapter_automater/cli_pipeline.py:#L62-L105`\n\n## System Initialization and Setup Wizard\nThe `setup_wizard.py` module handles the initial state of the system. It uses an interactive TUI to validate the environment. Despite presenting itself as a \"wizard,\" it functions as a hard gate for system dependencies like FFmpeg and GPU drivers.\n\nThe setup process follows a rigid state machine defined by the `SetupStep` Enum:\n1. `WELCOME`\n2. `SYSTEM_CHECK`\n3. `GPU_DETECTION`\n4. `INSTALLATION_TYPE`\n5. `DEPENDENCY_INSTALL`\n6. `CONFIGURATION`\n7. `VALIDATION`\n8. `COMPLETE`\n\nSources: `src/video_chapter_automater/setup_wizard.py:#L43-L52`\n\n## Pipeline Orchestration Flow\nWhen `vca-pipeline` is invoked, the `PipelineOrchestrator` manages the lifecycle of `Stage` objects. The flow is strictly sequential by default, though the code references a \"RESILIENT\" mode that supposedly continues on failure\u2014a design choice that suggests a tolerance for partial data that might be fucking useless for subsequent stages.\n\n```mermaid\nsequenceDiagram\n    participant CLI as cli_pipeline.py\n    participant ORC as PipelineOrchestrator\n    participant OM as OutputManager\n    participant STG as Stage\n\n    CLI->>ORC: execute(input_path)\n    activate ORC\n    ORC->>OM: ensure_structure()\n    loop For each StageConfig\n        ORC->>STG: execute(input_path)\n        STG-->>ORC: StageResult\n        alt Status == FAILED and stop_on_error\n            ORC-->>CLI: PipelineResult (Failure)\n        end\n    end\n    ORC->>ORC: _generate_manifest()\n    ORC-->>CLI: PipelineResult (Success)\n    deactivate ORC\n```\n\nSources: `src/video_chapter_automater/pipeline/orchestrator.py:#L53-L100`, `src/video_chapter_automater/pipeline/orchestrator.py:#L210-L235`\n\n## Configuration and Output Mapping\nThe CLI maps user flags to the `PipelineConfig` and `OutputManager`. The `OutputManager` creates a standardized directory structure, which is ironically documented via a `create_readme` method that writes a static string to a file\u2014a mechanism that ensures the user is told what the directories are, even if the processing fails to populate them.\n\n| Output Type | Directory | Description |\n| :--- | :--- | :--- |\n| `VIDEO` | `video/` | Re-encoded files (GPU accelerated). |\n| `AUDIO` | `audio/` | 16kHz mono WAV for transcription. |\n| `SCENES` | `scenes/` | Extracted images organized by video name. |\n| `METADATA` | `metadata/` | JSON manifests and statistics. |\n\nSources: `src/video_chapter_automater/output/manager.py:#L15-L35`, `src/video_chapter_automater/pipeline/config.py:#L66-L85`\n\n## Structural Observations\nThe CLI implementation reveals a heavy dependency on the `Rich` library for terminal presentation. In `cli.py`, the system attempts to import `Rich` and falls back to a `None` console, yet several subsequent logic paths assume a functional UI, creating a potential for runtime friction if the environment is stripped. \n\nFurthermore, the `PipelineOrchestrator` includes a `PARALLEL` execution mode labeled as a \"future enhancement,\" meaning the current CLI flags for parallel processing in `UserPreferences` are effectively cosmetic placeholders that do not influence the actual execution logic in the provided source.\n\nSources: `src/video_chapter_automater/cli.py:#L11-L24`, `src/video_chapter_automater/pipeline/orchestrator.py:#L88-L92`, `src/video_chapter_automater/setup_wizard.py:#L65-L75`\n\n## Conclusion\nThe CLI Reference defines a system that prioritizes structured output and user-facing feedback through TUI components. The interaction between `cli_pipeline.py` and `PipelineOrchestrator` ensures that video processing is treated as a series of discrete, configurable stages, while the `OutputManager` enforces a rigid file system hierarchy for all generated artifacts.",
      "filePaths": [
        "src/video_chapter_automater/cli.py",
        "src/video_chapter_automater/cli_pipeline.py"
      ],
      "importance": "high",
      "relatedPages": [
        "getting-started"
      ]
    }
  ]
}